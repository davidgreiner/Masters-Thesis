%% LaTeX2e class for student theses
%% sections/literature_review.tex
%% 
%% Karlsruhe Institute of Technology
%% Institute for Program Structures and Data Organization
%% Chair for Software Design and Quality (SDQ)
%%
%% Dr.-Ing. Erik Burger
%% burger@kit.edu
%%
%% Version 1.3.2, 2017-08-01

\chapter{Literature Review}
\label{ch:Literature-Review}

\section{Hand-Off Communication}
\label{sec:Literature-Review:Hand-Off Communication}
A hand-off in the medical community is the process of transferring a patient from one care provider to another \cite{SoletDJNorvellJMRutanGH2005}. During the hand-off process, key information is communicated, e.g., the patient’s state, administered medication, and treatment. The emergency medical service (\gls{EMS}) hand-off process consists of multiple stages; initially, patient information is communicated to the hospital trauma team during the transport of the patient. Once the patient arrives and care is transferred from EMS personnel to the hospital, more detailed information is communicated. Compared to the initial information the detailed information includes personal data, such as address, insurance information, previous injuries, etc. and a step-by-step list of treatment events. Finally, after the EMS arrive back at the rescue station a complete report of the patients treatment is finished and the process is complete \cite{Cohen2010}.
\par Inadequate communication is the leading cause of malpractice lawsuits; in 2015, three out of ten malpractice lawsuits mention a breakdown in communication \cite{CRICOStrategies.2015}. Doctors spend most of their time communicating with patients and other care providers, outside of performing surgeries. An observation of doctors for 35 hours and 13 minutes found that doctors engaged in communication events 78.7\% of the time \cite{Spencer2004}. When a patient arrives at the hospital, communication can take place in several different ways, such as verbal communication, verbal communication with note-taking, and exchange of a printed sheet with all of the information. Bhabra et al compared different hand-off communication methods found, that after five hand-off cycles communicating the same information, only communicating verbally has 97.5\% information loss, note-taking by the receiving person has 14.5\% information loss, and handing a printed sheet to the receiving person has 1.25\% information loss \cite{Bhabra2007}. The printed sheet was created by the person handing the patient over to the participant. The small information loss during communication using a printed sheet is explained by the amount of data a participant had to remember in this study, as the only data loss during the fifth cycle, while all previous cycles 100\% of the information was retained. Bhabra, Mackeith, Monteiro, and Pothier’s study focused on in-hospital care transfer and does not consider the level of acuity. A study by Meisel, Shea, Peacock, Dickinson, Paciotti, Bhatia, Buharin, and Cannuscio found that the quality of information exchange in the emergency department is higher than non-trauma patients \cite{Meisel2015}.
\par The Joint Commission on Accreditation of Healthcare Organizations made it its goal to improve medical hand-off communication in 2006 \cite{Arora2006}. The result of the commission’s studies found that the hand-off communication process is highly variable and discipline-specific. Therefore, every discipline and department developed a protocol specific for their unique requirements, using patient hand-off process maps and information checklists. A study comparing medical errors and preventable adverse events before and after the introduction of a standardized hand-off communication protocol for patient admission at nine pediatric residency training programs in the United States and Canada found that the medical error rate during the EMS hand-off process decreased by 23\% and the rate of preventable adverse events decreased by 30\% \cite{Starmer2014}. An automatic detection system can provide additional information that an \gls{EMS} personnel may have missed, further increasing the reporting accuracy and decreasing the medical error rate.
\par Hand-off communication in nursing is commonly done using a Electronic Health Record. Electronic Health Records store a patient's medical records and allow medical professionals to input vital data and share treatment information. A study has shown, that using these systems improved the continuity of care and increased the consistency of data \cite{COLLINS2011704}. The existing Electronic Health Record can be updated by an automatic detection system to provide real-time information about the care the patient receives from the time the EMS personnel arrive at the scene until the hand-off at the hospital.
\par The hand-off communication between \gls{EMS} personnel and the hospital's trauma team presents unique challenges. Both participants have different worksites and their own clinical duties, which may result in communication errors due to the participants being unfamiliar with procedures implemented by hospitals and EMS. The time window during which the communication takes place is extremely short. Information communication during transport is limited due to the urgency to provide care for the patient. Automatically detecting procedures may augment the verbally communicated information and may be sent in real-time through a different communication modality. Depending on the patient's level of acuity emergency department staff may pay less attention during the handoff communication if the injuries are deemed non-life-threatening \cite{SoletDJNorvellJMRutanGH2005,Meisel2015}. If a patient with a common injury arrives, emergency department staff may assume they already know everything about the case therefore, potentially missing out on critical information resulting in permanent injury or death.

\section{Human Activity Recognition}
\label{sec:Literature-Review:Human-Activity-Recognition}
\emph{Human Activity Recognition} (\gls{HAR}) is used to recognize common human activities in real life settings \cite{Helal2010}. HAR uses patterns discovered from low-level sensor data to train activity models, which detect the human's activity.
\par Increasing interest in the field of HAR has lead to improved computational power, smaller size, and lower cost of sensors \cite{Rodgers2015}. The sensors used to recognize human activities can be split into two categories: \emph{external} and \emph{wearable} sensors \cite{Lara2013}. An external sensor observes the human from a fixed point of view and relies on human interaction in its field of view. A wearable sensor observes the human while being attached to the body. The reviewed sensor types and their advantages and disadvantages for  their use to automatically detect EMS procedures are provided in Table \ref{har-sensors}.
\begin{table}[]
	\centering
	\caption{Summary of Human Activity Recognition sensors}
	\label{har-sensors}
	\begin{tabularx}{\textwidth}{|c|X|X|}
		\hline
		\thead{Sensor} & \thead{Pros} & \thead{Cons}  \\\hline
		\makecell{Camera} & \makecell{Captures all body parts of\\the human\\Not worn on the body\\Higher information flow\\Capture information around\\the human} & \makecell{Requires human to be in\\field of view\\Higher privacy invasion\\Computational expensive\\Requires good lighting} \\\hline
		\makecell{Environmental} & \makecell{Senses the environmental\\context of the human} & \makecell{Not very accurate for HAR} \\\hline
		\makecell{Acceleration} & \makecell{Most accurate wearable sensor\\for HAR} & \makecell{Sensor placement can\\make a difference} \\\hline
		\makecell{Location} &\makecell{Useful for detecting\\transportation} & \makecell{Not useful for fine-grained\\detection} \\\hline
		\makecell{Physiological} & \makecell{Useful for measuring human's\\activity load} & \makecell{Activity load is different\\depending on fitness level} \\\hline
	\end{tabularx}
\end{table}

\subsection{External Sensors}
Extensive \gls{HAR} research has focused on using cameras as external sensors to visually recognize gestures and movement. Visual recognition has two primary sources of data: RGB and grayscale video. RGB data is the visual representation of the camera in pixel of red, green, and blue values. Grayscale data is the depth representation of the camera in a shade of gray, white being closest and black being the furthest away. Multiple features can be extrapolated from the video data, e.g. body part detection, motion detection. A common feature for visual recognition uses background subtraction to project a silhouette onto a person, which allows for the detection of a region of interest where motion occurs \cite{Bobick2001}. Isolating this motion allows for a more detailed analysis of the region of interest. Motion around the patient in the context of an ambulance  indicates, that a procedure is currently being performed. Using multiple cameras, it is possible to generate 3D models with silhouettes \cite{Weinland2006}. 3D models are used to generate information about the positioning of limbs, the distance relative to other objects, and the humans pose.
\par Regions of interest can also be detected using the motion information of the video. The change between video is analyzed pixel-wise to determine the direction of the movement, making it useful for detecting moving objects, while the camera is steady \cite{Efros03}. Humans are rarely stationary, therefore the motion information can be used to generate features, such as the human's speed.
\par Joint or joint angles detection is more complex than detecting silhouettes. Although, joint detection provides a richer dataset to a HAR algorithm, which increases the algorithm’s accuracy \cite{CGV-005}. Joint detection requires a 3D scene in order to keep the representation view-invariant \cite{Poppe2010}. Using joints and their angles allows for reconstructing and identifying the human skeleton. Understanding the human skeletal position allows for more accurate classifications of skeletal-based human activities, such as waving an arm, jumping, or sitting.

\subsection{Wearable Sensors}
Wearable sensors come in all shapes and sizes. Most people never notice how many sensors are carried around in their personal devices on a daily basis. Wearable sensors for human activity recognition fall into one of four groups: environmental attributes, acceleration, location, and physiological signals \cite{Lara2013}. Sensors for environmental attributes do not collect data about the human, but rather about the human's surrounding environment. An acceleration sensor senses the movement of the body part on which its placed, relative to the whole body movement. Location sensors determine the location of the human as coordinates on a map. Physiological signals sensors monitor the human's vital statistics.
\par Environmental attributes relate to the surrounding of the human wearing the sensor, e.g., temperature, noise level, and light intensity. These environmental attributes allow for the detection of the climate the human is currently experiencing. For example, using the light intensity, weather information, and time of day, an approximate location for the human can be determined due to climate differences on earth. Depending on the environmental attributes, human may be more likely to take part in certain activities, such as walking in sunshine and warm weather. Environmental attributes information may not be sufficient to solely recognize a human activity.
\par The most broadly used sensors in HAR are the accelerometer and gyroscope sensors \cite{Lara2013}. The acceleration and gyroscope sensors are commonly found in almost every smart device, e.g., smartphones, smart watches, fitness trackers. The placement of the sensor on the human body is important for accurately detecting the human's activity, as body parts move differently depending on the activity. For example, movements while sitting cannot be captured if the sensor is placed in the human's pockets \cite{Maurer2006}.
\par Location information is usually obtained using a GPS sensor, which is interesting for moving humans as their means of transportation can be detected using the human's speed \cite{Zheng2008}. Additionally, the geographical context helps infer the activity a human is participating in \cite{liao2006location}. For example, if a person is currently in a school and not moving, then the human is either a student or a teacher sitting at a desk. Using this location information a smartphone may automatically be turned off for a student when the students enters the school building.
\par Finally, physiological signals, e.g., heart rate, respiration rate, and skin temperature, can be used alongside acceleration data to more accurately predict human activity \cite{Lara2012}. Activities may have different effects on a human's vitals. For example, the more demanding the activity, the higher the heart and the lower the respiration rate.

\subsection{Differences}
External and wearable sensors differ in regards to privacy, pervasiveness, complexity, mobility, and accuracy. These factors are important for developing a system to detect trauma procedures by EMS personnel. Privacy is extremely important in a health care environment, which is why the United States has HIPAA in place. Pervasiveness determines how much information a sensor can acquire at any given time. For for an automatic detection system to work effectively, the system needs to run in real-time; thus, the system needs low pervasiveness sensors. EMS personnel are usually mobile while performing their duties from picking up a patient to transporting them to the ambulance. Finally, sensor accuracy is critical, as a falsely recognized EMS procedure may have fatal results.
\par Privacy has become a growing concern in society \cite{privacyindex}. Not only the patient \cite{Arning2015}, but EMS personnel may object to constant monitoring using cameras, as the video may be used to prove malpractice. Over 40\% of police leadership think that body-worn cameras are used to "fish" for evidence against their officers \cite{Smykla2016}. The same problem may occur when EMS personnel are equipped or surrounded by cameras, as the video can be used as evidence of wrong-doing. Wearable sensors may reduce this perception of privacy intrusion, as the wearable sensor is a passive observer without the on-looking lens.
\par Pervasiveness for external sensors in the ambulance environment can be difficult. The amount of EMS personnel working on a patient increases the probability that a region of interest will be obscured, which reduces the amount of data that can be collected. Although, the use of several cameras at different positions and angles partially mitigates the obscureness affect. Wearable sensors are able to capture data the entire time the sensor is worn, while cameras collect more informative data concerning location that wearable sensors may not be able to detect.
\par Video processing is extremely computational expensive; therefore, external sensors have a high complexity. A Full HD camera with 30 frames per second using the H.264 codec has a data rate of about 8.2 Mbps. The video data is then processed to detect the human skeleton using software, such as OpenPose \cite{openpose}. Real-time video processing requires a high end graphics card. Wearable sensors capture less data; thus, the wearable sensors have lower complexity than external sensors.
\par Cameras for HAR are typically stationary and are positioned ahead of time to cover a region of interest. Everything outside of the camera’s view is not sensed. Therefore, the mobility of a video monitoring system is low. The mobility of a camera is negligible in the context of detecting procedures in an ambulance, as the environment does not change. Wearable sensors commonly use wireless technology, e.g., Bluetooth, to communicate with a processing computer. The mobility of the wearable sensors allows the wearer to have the same range of movement as without it. A downside of wearable sensors' mobility is battery-life. Depending on the device, the battery capacity might be limited and not last through its designed time. For detecting procedures performed outside of the ambulance, wearable sensors are essential. Wearable sensors can connect to a phone on the EMS personnel and collect data anywhere.
\par Accuracy of sensors depend on two categories: hardware and processing. First the hardware of the sensor has to collect the data accurately. Several factors of cameras determine the accuracy of the data, such as resolution, camera sensor size, low-light capabilities, etc. Wearable sensor accuracy depends on the build quality and frequency at which data is obtained. Data has to be processed in an intelligent way, in order to make sense of it. Machine Learning systems are able to take data and train a model to recognize human activities. The most recent visual-based activity recognition can accurately detect between 26.9\% and 70.4\% \cite{Herath2017} human activities from the human motion database \cite{Kuehne11}. Every day wearable sensors such as our smartphones can detect jogging, laying, sitting, standing, and walking with a 99.01\% accuracy \cite{Wannenburg2016}. Wannenburg and Malekian's \cite{Wannenburg2016} results are not comparable to Herath, Harandi, and Porikli's \cite{Herath2017} result as the datasets were different, but Wannenburg and Malekian’s results show how far existing Machine Learning algorithms have come in recent years.
\par Visual-based activity recognition can vary greatly in accuracy as many external factors, such as lighting can affect the quality of video data. Wearable sensors and external sensors complement each other well. While visual data are able to capture multiple humans at once, a wearable sensor is limited to one person and requires multiple sensors to capture each person. Wearable sensors are highly sensitive to the body part they are placed on, as accelerometer and gyroscope data will only be able to capture the movement of the monitored limb.
\par Wearable sensors are an effective tool at recognizing human activity with a low intrusion of privacy, low computational expensiveness, and their pervasiveness.

\section{Machine Learning within Human Activity Recognition}
\label{sec:Literature-Review:Machine-Learning-Classifiers}
Machine learning algorithms are trained on data collected from wearable and external sensors in order to recognize human activity. For the purpose of detecting procedures performed by EMS personnel inside an ambulance, the body part movements are divided into two categories: coarse-grained and fine-grained movements. Coarse-grained movements are the broadest way to describe an activity, such as walking or jogging. Fine-grained movements are the anatomic movements involved to perform an activity, such as lifting a foot, moving the foot forward, and setting the foot down. The approaches of detecting each movement type are analyzed by examining how the features of the data-set were extracted and what learning methods were used to generate an HAR model. Activity recognition accuracies are compared to determine the effectiveness of a machine learning algorithm for \gls{HAR}. The different machine learning algorithms for HAR, their number of features, number of activities detected, the kind of sensors used, and the accuracy of the algorithm are listed in Table \ref{machine-learning-summary}.
\begin{table}[h]
	\centering
	\caption{Summary of Machine Learning Algorithms within Human Activity Recognition}
	\label{machine-learning-summary}
	\begin{tabularx}{\textwidth}{|c|c|c|c|X|c|}
		\hline
		\thead{Algorithm} & \thead{Paper} & \thead{Features} & \thead{Activities} & \thead{Sensors used} & \thead{Accuracy}   \\\hline
		\makecell{$k$-NN ($k=1$) \\ $k$-Star} & \cite{Wannenburg2016}  & 29         & 5 & \makecell{accelerometer} & 99.01\% \\\hline
		\makecell{sparse \\ representation} & \cite{Zhang2013} & 60 & 9 & \makecell{accelerometer \\ gyroscope \\ magnetometer} & 96.1\% \\\hline\hline
		random forest & \cite{Dorfmeister2014} & 80 & 17 & \makecell{accelerometer \\ gyroscope \\ magnetometer \\ barometer \\ GPS \\ microphone} & 90\% \\\hline
		$k$-NN ($k=5$) & \cite{Benalcazar2017}  & 8 & 5 & \makecell{accelerometer \\ gyroscope \\ magnetometer \\ EMG} & 86\% \\\hline
		$k$-NN ($k=3$) & \cite{Totty2017}  & 3 & 17 & \makecell{accelerometer \\ gyroscope \\ EMG} & 89.2\% \\\hline
	\end{tabularx}
\end{table}
\subsection{Coarse-grained movements}
A study by Wannenburg and Malekian (2016) detected everyday physical activities from smartphone accelerometer data using a $k$-nearest neighbor ($k$-NN) and $k$-Star machine learning algorithm \cite{Wannenburg2016}. Five different activities were defined to be recognized: standing, sitting, laying down, walking, and jogging. The data was obtained using a three-axis accelerometer sensor of a smartphone placed in ten participant's pants pockets. Windowing with a size of 1s and 50\% overlap was applied, after normalizing the data. 46 features were extracted, such as min, max, mean, and median of every axis, as well as the signal magnitude area (\gls{SMA}). The 29 highest contributing features were then selected to train ten different classifiers. $k$-NN ($n = 1$) and $k$-Star achieved the highest classification accuracy with 99.01\%. Wannenburg and Malekian’s study only detected activities related to daily life. The algorithm's limited amount of activities recognized is not usable for automatically detecting procedures administered by EMS. The algorithm's high accuracy signifies a good feature extraction and training process, but the activities have to be extended to include more coarse-grained movements related to administering EMS procedures.
\par A three-axis accelerometer, three-axis gyroscope, and three-axis magnetometer were used by Zhang and Sawchuk (2013) to recognize nine common activities \cite{Zhang2013}. Fourteen participants performed nine activities: walk forward, walk left, walk right, go upstairs, go downstairs, jump up, run, stand, and sit. The sensors were placed on the hip of every participant and 110 features were extracted, such as mean, median, variance, SMA, etc. Features were selected using sequential forward selection for four classifiers: $k$-nearest neighbor, naive Bayesian classifier, support vector machine, and sparse representation. The sparse representation classifier achieves the highest accuracy (96.1\%), when using 60 features.
\par The studies of Zhang and Sawchuk, and Wannenburg and Malekian achieved a high accuracy at detecting common human activities in daily life. The activities detected in Zhang and Sawchuk's work were broader than the activities detected by Wannenburg and Malekian's work. The features extracted were similar, such as using min, max, SMA, etc. Therefore, using accelerometer, gyroscope, and magnetometer to detect common human activities in daily life is highly accurate. The same process of processing data of coarse-grained movement can be applied for recognizing procedures administered by EMS personnel.
\subsection{Fine-grained movements}
A machine learning algorithm developed by Maier and Dorfmeister (2014) detected 17 different fine-grained movements related to subway travel \cite{Dorfmeister2014}. The sensors used were accelerometer, gyroscope, magnetometer, barometer, GPS, and microphone data. A window of 2048 ms with 50\% overlap was applied to the sensor data in order to achieve the highest classifier accuracy. Windows with multiple labels resulting from differently tagged activities overlapping due to the windowing procedure were removed. The Fast-Fourier transformation was applied to the sensor data to compute frequency-based features, while time-based features, i.e., the maximum and the mean, were also computed to generate a total of 632 features. A correlation-based feature subset selection was used to reduce the 632 features to 80 features. The random forest classifier achieved a 90\% accuracy with the parameters set to their default values. This study proves that fine-grained movements can be accurately detected using the sensors in a smartphone. Microphone data must not be included in a health care environment due to privacy concerns.
\par Detecting hand gestures using the Myo armband was done by Benalc{\'{a}}zar, Jaramillo, Zea, and P{\'{a}}ez \cite{Benalcazar2017}. Hand gestures detected by the Myo were: fist, open hand, wave hand in, wave hand out, pinch fingers, and no gesture. Electromyography (\gls{EMG}) data was captured at 200 Hz and pre-processed by taking the absolute value of all of the EMG channels followed by a Berutterworth filter to reduce noise and smooth each channel. A 2 second window with an overlap of 50\% was applied to the EMG data, which generates 400 samples. The \emph{k}-nearest neighbor rule and the dynamic time warping algorithm was used to recognize the hand gestures and achieved an accuracy of 86\%. The included proprietary hand gesture recognition of the Myo only achieves an accuracy of 83\%. The process of processing the data may be useful for detecting fine-grained movements by EMS personnel while performing procedures on patients.
\par Using the Myo armband, Totty and Wade (2017) were able to train a $k$-NN machine learning algorithm to detect upper-extremity activities \cite{Totty2017}. Gestures were categorized and split into tasks with an approach used by the Functional Arm Activity Behavioral Observation System \cite{FAABOS}. Ten participants performed 17 upper-extremity tasks, such as scratching leg, reaching, grabbing, waving, etc. The accelerometer and gyroscope data were smoothed using a 4th order Butterworth band-pass filter and the EMG data was high-pass filtered. Features included the mean and the \gls{SMA} of the acceleration and gyroscope data, and the root mean square (\gls{RMS}) of the \gls{EMG} data. Data from the magnetometer was not used due to its susceptibility to environmental noise \cite{Ahmad2013}. The $k$-NN classifier achieved an accuracy of 89.2\%.
\par Compared to the first Myo study, Totty and Wade detected activities that may be useful in the context of medical procedure detection. The feature extraction and pre-processing is a starting point for data collected in a study.
\subsection{Intention Recognition}
\par Using the sequence of fine-grained movements, coarse-grained movements can be recognized \cite{Dirk2010}. Multiple fine-grained movements in a certain order result in a coarse-grained movement. For example, EMS personnel placing an oral airway (coarse-grained movements) have to place thumb on bottom teeth and index finger on upper teeth of patient, then move the fingers outward, grab an airway, insert it into the patients mouth, and finally rotate the airway 180 degrees.
\par The recognition of a coarse-grained movement can be successful before all fine-grained movements have been completed, which is called intention recognition \cite{Schrempf2005}. Each coarse-grained movement is modeled using a Hidden Markov Model (HMM). Gehrig, Krauthausen, Rybok, K{\"{u}}hne, Schultz, Hanebeck, and Stiefelhagen used a Hybrid Dynamic Bayesian Network to recognize the intended coarse-grained movement \cite{Gehrig2011}. The probability density is represented using continuous density functions. The intention recognition system was tested using seven kitchen activities captured on video, such as lay table, prepare cereals, prepare pudding, eat with spoon, eat with fork, clear table, and wipe table. 74.4\% was the average recognition rate for the evaluation test set. An intention recognition system can send the hospital information about the current procedure that is performed on the patient while it is still in progress. Time critical information about the current treatment allows the emergency department to prepare accordingly before the arrival of the patient.
\subsection{Discussion}
Several algorithms exists to accurately recognize human activities. The highest accuracy was achieved by Wannenburg and Malekian \cite{Wannenburg2016} with 99.01\%. A fair comparison has to take the amount of features and number of detected activities into consideration. More detected activities usually mean lower accuracy. The algorithm by Maier and Dorfmeister \cite{Dorfmeister2014} has a good balance between number of activities (17) and amount of features (80), while still achieving a high accuracy at 90\%.
\par Most algorithms followed the same procedure for feature extraction and processing data. This procedure of taking a window with 50\% overlap, reducing noise with a 4th order Butterworth band-pass filter, and calculating mean, median, variance, SMA, etc. can be applied to data for automatically recognizing procedures administered by EMS personnel.
\par The Myo is useful for detecting muscle activation on a human's arm. Additionally, the Myo includes acceleration, gyroscope, and magnetometer data which have proven accurate in detecting human activities. The studies listed above have similar approaches to feature extraction and data processing, which can also be applied in the context of medical procedure detection.

\section{Summary}
\label{sec:Literature-Review:Summary}
Different sensors were examined and compared for their applicability in the medical field. Combining an external sensor with wearable sensors may achieve the highest accuracy for detecting human activities. Human activities consist of coarse-grained and fine-grained movements. Several machine learning algorithms using wearable sensors exist accurately detecting human actives. Inferring coarse-grained movements by using fine-grained movements may be useful in the context of detecting procedures administered by EMS personnel inside an ambulance. 