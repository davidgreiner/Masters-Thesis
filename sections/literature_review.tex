%% LaTeX2e class for student theses
%% sections/literature_review.tex
%% 
%% Karlsruhe Institute of Technology
%% Institute for Program Structures and Data Organization
%% Chair for Software Design and Quality (SDQ)
%%
%% Dr.-Ing. Erik Burger
%% burger@kit.edu
%%
%% Version 1.3.2, 2017-08-01

\chapter{Literature Review}
\label{ch:Literature-Review}

\section{Hand-Off Communication}
\label{sec:Literature-Review:Hand-Off-Communication}
A hand-off in the medical community is the process of transferring a patient from one care provider to another \cite{SoletDJNorvellJMRutanGH2005}. During the hand-off process key information is communicated, e.g., the patient’s state, administered medication, and treatment. The emergency medical service (\gls{EMS}) hand-off process consists of multiple stages. Initially, patient information is communicated to the hospital trauma team during patient transport. Once the patient arrives, and care is transferred from the EMS personnel to the hospital, more detailed information is communicated. Compared to the initial information, the detailed information includes personal data, such as address, insurance information, previous injuries, etc. and a step-by-step list of treatment events. Finally, after the EMS return to the rescue station, a complete report of the patient's treatment is compiled, and the process is complete \cite{Cohen2010}.
\par Inadequate communication is the leading cause of malpractice lawsuits; in 2015, three out of ten malpractice lawsuits mention a breakdown in communication \cite{CRICOStrategies.2015}. Doctors spend most of their time communicating with patients and other care providers. An observation of doctors for 35 hours and 13 minutes found that doctors engaged in communication events 78.7\% of the time \cite{Spencer2004}. When a patient arrives at the hospital, communication between hospital staff can take place in several different ways. Bhabra, Mackeith, Monteiro, and Pothier \cite{Bhabra2007} compared information loss during hand-off for three communication methods: verbal communication, verbal communication with note-taking, and exchange of a printed treatment record by the person handing off the patient to the person assuming the responsibility of care. Every participant received the same communication in every cycle. After five cycles the information loss when communicating verbally was 97.5\%, note-taking by the receiving person incurred a 14.5\% information loss, and handing a printed sheet to the receiving person resulted in 1.25\% information loss \cite{Bhabra2007}. The small information loss during communication using a printed sheet was due to the amount of information a participant had to remember. The only information loss when exchanging a printed sheet occurred during the fifth cycle, while 100\% of the information was retained during all previous cycles. Bhabra et al.’s study focused on in-hospital care transfer and did not consider time critical or trauma situations. A study of hand-offs between EMS personnel and emergency department staff found that the quality of information exchange in the emergency department is higher for trauma patients than for non-trauma patients \cite{Meisel2015}. EMS personnel reported that when handling trauma cases, the emergency department staff were highly engaged and desired a detailed report, while emergency department staff were less interested in non-trauma patients.
\par The Joint Commission on Accreditation of Healthcare Organizations set a goal to improve medical hand-off communication \cite{Arora2006}. The commission conducted a study and found that the hand-off communication process is highly variable and discipline-specific. Therefore, every discipline and department developed a protocol specific to their unique requirements. A study comparing medical errors and preventable adverse events before and after the introduction of a standardized hand-off communication protocol for patient admission at nine pediatric residency training programs in the United States and Canada was conducted. The study found that the medical error rate during the EMS hand-off process decreased by 23\%, and the rate of preventable adverse events decreased by 30\% after the implementation of a standardized hand-off communication protocol \cite{Starmer2014}. An automatic detection system can provide additional information that an \gls{EMS} personnel may have missed, further increasing the reporting accuracy and decreasing the medical error rate.
\par Hand-off communication in nursing is commonly done using an Electronic Health Record. Electronic Health Records store a patient's medical records and allow medical professionals to input vital data and share treatment information. It was determined, that using these systems improved the continuity of care and increased the consistency of data \cite{COLLINS2011704}. The existing Electronic Health Record can be updated using an automatic detection system to provide real-time information regarding a patient's care, from the time the EMS personnel arrive at the scene until the hand-off at the hospital.
\par The hand-off communication between \gls{EMS} personnel and the hospital's team presents unique challenges. Both sets of personnel have different worksites and clinical duties, which may result in communication errors due to the personnel being unfamiliar with each others' procedures. The time window during which the communication occurs is extremely short. Information communication during transport is limited due to the urgency to provide care to the patient. Automatically detecting procedures may augment the verbally communicated information and may be sent in real-time through a different communication modality. Depending on the patient's level of acuity, emergency department staff may pay less attention during the hand-off communication, if the injuries are deemed non-life-threatening \cite{SoletDJNorvellJMRutanGH2005,Meisel2015}. If a patient with a common injury arrives, emergency department staff may assume they already know everything about the case; therefore, potentially missing critical information and resulting in permanent injury or death.

\section{Human Activity Recognition}
\label{sec:Literature-Review:Human-Activity-Recognition}
\emph{Human Activity Recognition} (\gls{HAR}) is a method intended to recognize common human activities in real life settings \cite{Helal2010}. HAR uses patterns discovered from low-level sensor data to train activity models that can be used to detect the human's activity.
\par Increasing interest in HAR has lead to improved computational power, smaller size, and lower cost of sensors \cite{Rodgers2015}. The sensors used to recognize human activities fall into two categories: \emph{external} and \emph{wearable} sensors \cite{Lara2013}. An external sensor observes the human from a fixed point of view and relies on human interaction within the sensor's range and field of view. A wearable sensor is worn on the user's body and collects information as the human conducts their activities. The reviewed sensor types and the anticipated associated advantages and disadvantages for automatically detecting EMS procedures are provided in Table \ref{har-sensors}. The following chapter examines the advantages and disadvantages for each sensor and how they can be used to automatically detect EMS procedures.
\begin{table}[]
	\centering
	\caption{Summary of Human Activity Recognition sensors}
	\label{har-sensors}
	\begin{tabularx}{\textwidth}{|c|X|X|}
		\hline
		\thead{Sensor} & \thead{Advantages} & \thead{Disadvantages}  \\\hline
		\makecell{Camera} & \makecell{Captures all body parts of\\the human\\Not worn on the body\\Higher information flow\\Capture information around\\the human} & \makecell{Requires human to be in\\field of view\\Higher privacy invasion\\Computational expensive\\Requires good lighting} \\\hline
		\makecell{Environmental} & \makecell{Senses the environmental\\context of the human} & \makecell{Not very accurate for HAR} \\\hline
		\makecell{Acceleration} & \makecell{Most accurate wearable sensor\\for HAR} & \makecell{Sensor placement can\\make a difference} \\\hline
		\makecell{Location} &\makecell{Useful for detecting\\transportation} & \makecell{Not useful for fine-grained\\detection} \\\hline
		\makecell{Physiological} & \makecell{Useful for measuring human's\\activity load} & \makecell{Activity load is different\\depending on fitness level} \\\hline
	\end{tabularx}
\end{table}

\subsection{External Sensors}
Extensive \gls{HAR} research has focused on using cameras as external sensors to visually recognize gestures and movement. Visual recognition has two primary sources of data: RGB and grayscale video. RGB data is the visual representation of the camera in pixels of red, green, and blue values. Grayscale data is the depth representation of the camera in a shade of gray, white being closest and black being the furthest away. Multiple features can be extrapolated from the video data, e.g., body part detection \cite{Chakraborty2008}, motion detection \cite{4813430}. A common analysis approach uses background subtraction to project a silhouette onto a person, which allows for the detection of a region of interest within which motion occurs \cite{Bobick2001}. Isolating this motion permits a more detailed analysis of the region of interest. Motion around the patient, in the context of an ambulance, indicates that a procedure is likely being performed. Using multiple cameras, it is possible to generate 3D models with silhouettes of humans \cite{Weinland2006}. 3D models are used to generate information regarding the positioning of human limbs, the human's distance relative to other objects, and the human's pose.
\par Regions of interest can be detected using motion information in the video. The difference between video frames is analyzed pixel-wise to determine the direction of the movement, making it useful for detecting moving objects when the camera is steady \cite{Efros03}. Humans are rarely stationary; therefore the motion information can be used to generate features, such as the human's speed.
\par Joint angle detection is more complex than detecting silhouettes. Joint detection provides a richer dataset to a HAR algorithm, which can increase the algorithm’s accuracy \cite{CGV-005}. Joint detection requires a 3D scene to maintain a representation that is view-invariant \cite{Poppe2010}. Using joints and their angles allows for reconstructing and identifying the human skeleton. Understanding the human skeletal position allows for more accurate classifications of skeletal-based human activities, such as waving an arm, jumping, or sitting \cite{HsuanSheng}.

\subsection{Wearable Sensors}
Wearable sensors come in all shapes and sizes. Most people never notice how many sensors are carried around in their devices on a daily basis. Wearable sensors for human activity recognition fall into one of four groups: environmental attributes, acceleration, location, and physiological signals \cite{Lara2013}. Sensors for environmental attributes do not collect data about the human, but rather about the human's surrounding environment \cite{Juha2006}. A sensor for environmental attributes is the air pressure sensor which measures the altitude of the human \cite{5559476}, or the humidity sensor which measures the amount of amount of water vapor in the air \cite{Qin2007}. An acceleration sensor senses the movement of the body part on which its placed, relative to the whole body movement \cite{Maurer2006}. An acceleration senors may contain up to three axes (X,Y,Z). A popular use for acceleration sensors in smartphones is step counting of the human's activity \cite{Brajdic:2013}. Location sensors determine the location of the human as coordinates on a map \cite{Quddus2003}. For example, a Global Positioning System (GPS) sensor uses satellites in space to triangulate a human's position to latitude, longitude, and elevation \cite{hofmann2012global}. Physiological signals sensors monitor the human's vital signs \cite{Yin:2008}. For example, heart rate, respiration rate, skin temperature, and electrocardiogram amplitude can be used to improve activity recognition \cite{Lara2012}.
\par Environmental attributes relate to the human's surroundings, e.g., temperature, noise level, and light intensity \cite{Maurer2006}. These environmental attributes allow for detecting the climate the human is currently experiencing. For example, using the air pressure an approximate location of the human in a subway system can be determined \cite{van2017subwayapps}. Depending on the environmental attributes, humans may be more likely to take part in certain activities, such as walking in sunshine and warm weather. Environmental attribute information may be insufficient as the sole means of recognizing human activity.
\par The most broadly used HAR sensors are the accelerometer and gyroscope sensors \cite{Lara2013}. The acceleration and gyroscope sensors are commonly found in almost every smart device, e.g., smartphones \cite{5560598}, smartwatches \cite{Shen:2016}, fitness trackers \cite{Kaewkannate2016}. The placement of the sensor on the human body is important for accurately detecting the human's activity, as body parts move differently depending on the activity. For example, movements while sitting cannot be captured if the sensor is placed in the human's pockets \cite{Maurer2006}.
\par Location information is usually obtained using a GPS sensor, which is interesting for moving humans, as their means of transportation can be detected using the human's speed \cite{Zheng2008}. Additionally, the geographical context helps infer the activity in which the human is engaged \cite{liao2006location}. For example, 30\% of traffic in urban areas are due to drivers searching for a parking spot. A system using GPS sensors can detect when a spot has vacated and alert a nearby driver \cite{Nawaz:2013}.
\par Finally, physiological signals, e.g., heart rate, respiration rate, and skin temperature, can be used alongside acceleration data to more accurately predict human activity \cite{Lara2012}. Activities may have different effects on a human's physiological metrics \cite{Lara2013}. For example, the more demanding the activity, the higher the heart-rate and the lower the respiration rate \cite{Lara2013}.
\par Recognizing human activities from wearable sensors requires data acquisition. Data is acquired in multiple stages \cite{Lara2013}. First, wearable sensors are selected for their suitability to detect a certain actions or activities representative of a task. Second, the wearable sensors are integrated with devices that capture the data. Finally, the data is stored either locally on the integration device or remotely on a server. During the data acquisition process subjects perform a predefined set of activities that the recognition system is later able to detect. After the data is collected, activity recognition consists of two stages, training and testing the activity recognition system, which is described in Chapter \ref{sec:Literature-Review:Machine-Learning-Classifiers}.

\subsection{Differences}
External and wearable sensors differ in regards to privacy, pervasiveness, complexity, mobility, and accuracy. These factors are important for developing a system to detect the medical procedures EMS personnel apply to a patient. Privacy is extremely important in a healthcare environment, which is why the United States has HIPAA \cite{HIPAA} and Germany has "Die \"artzliche Schweigepflicht" \cite{Privatgeheimnissen}. Pervasiveness is defined as the sensor's ability to be connected or attached to any device and any location \cite{Lara2013}. Proper functioning of the automatic detection system requires the system must run in real-time. EMS personnel are usually mobile when performing their duties from initiating patient care to transporting and hand-off at the hospital. Finally, sensor accuracy is critical, as a falsely recognized EMS procedure may have fatal results \cite{mourcou2015performance}.
\par Privacy has become a growing societal concern \cite{privacyindex}. Patients and EMS personnel may object to constant monitoring using cameras \cite{Arning2015}. Over 40\% of police leadership think that body-worn cameras are used to "fish" for evidence against their officers \cite{Smykla2016}. The same problem may occur when EMS personnel are equipped or surrounded by cameras, as the video may be used as evidence of wrong-doing. Wearable sensors may reduce this perception of privacy intrusion, as the wearable sensor is a passive observer without the camera.
\par Pervasiveness for external sensors is low, because cameras can not be easily attached to humans. The cameras have to be mounted externally and pointed at the human. The monitored human has to stay within a perimeter defined by the position and the field of view of the camera. Inside an ambulance multiple cameras have to be deployed to mitigate the obscureness effect of body parts covering a region of interest \cite{howe2000bayesian}. Wearable sensors can capture data the entire time the sensor is worn, while cameras collect more informative data concerning the overall environment and activities that wearable sensors may be unable to detect.
\par Video processing is extremely computational expensive; therefore, external sensors have high complexity. A Full HD camera with 30 frames per second using the H.264 codec has a data rate of about 8.2 Mbps. The video must be processed to identify the human skeleton using software, such as OpenPose \cite{openpose}. Real-time video processing requires a high-end graphics card \cite{732851}. Wearable sensors capture a different type of data at a different resolution; thus, the wearable sensors have lower complexity than external sensors.
\par HAR cameras are typically stationary and positioned a priori in order to cover a region of interest \cite{Poppe2010}. Therefore, the mobility of a video monitoring system is low. A camera's mobility is negligible in the context of detecting procedures in an ambulance, as the environment does not change. Wearable sensors commonly use technology, e.g., Bluetooth, WiFi, to communicate with a processing computer \cite{Lara2013}. The mobility of the wearable sensors allows the wearer to maintain his or her range of motion. A downside of wearable sensors' mobility is battery-life \cite{Lara2013}. Depending on the device, the battery capacity may be limited and not last through its designed time. Wearable sensors are essential for detecting procedures performed outside of the ambulance. Wearable sensors can connect to a phone on the EMS personnel and collect data anywhere \cite{Lara2013}.
\par The accuracy of sensors is dependent on the sensors' hardware and processing. First, the sensor's hardware has to collect the data accurately. Several factors of cameras determine the data's accuracy, such as resolution \cite{doermann2003progress}, camera sensor size \cite{6712704}, low-light capabilities \cite{1315150}, etc. Wearable sensor accuracy depends on the sensors' uncertainty of measurement and sampling rate at which data is obtained. Machine Learning systems can use data to train a model to recognize human activities. Recently, visual-based activity recognition accurately detected up to 70.4\% \cite{Herath2017} of human activities from the human motion database \cite{Kuehne11}. Every day wearable sensors, such as smartphones can detect jogging, laying, sitting, standing, and walking with a 99.01\% accuracy \cite{Wannenburg2016}. Wannenburg and Malekian's \cite{Wannenburg2016} results are not comparable to Herath, Harandi, and Porikli's \cite{Herath2017} result as the datasets were different, but Wannenburg and Malekian showed how far existing Machine Learning algorithms have come in recent years.
\par Visual-based activity recognition can vary greatly in accuracy, as many external factors, such as lighting can affect the quality of video data \cite{1315150}. Wearable sensors and external sensors complement each other well \cite{5482111}. While visual data can capture multiple humans simultaneously, a wearable sensor is limited to a single individual and requires multiple sensors to capture each person. Wearable sensors are highly sensitive to the body part on which they are placed, as accelerometer and gyroscope data will only capture the movement of the monitored limb.
\par Wearable sensors are an effective tool for recognizing human activity with a low intrusion of privacy \cite{Lara2013}, low computational expensiveness \cite{Lara2013}, and their pervasiveness \cite{Lara2013}.

\section{Machine Learning within Human Activity Recognition}
\label{sec:Literature-Review:Machine-Learning-Classifiers}
Machine learning algorithms are trained on data collected from wearable and external sensors to recognize human activity. The body part movements are divided into two categories for the purpose of detecting procedures performed by EMS personnel inside an ambulance: coarse-grained and fine-grained movements. Coarse-grained movements are the broadest way to describe an activity, such as cutting an apple \cite{Dirk2010}. Fine-grained movements are the anatomic movements involved to perform an activity, such as picking up apple, placing apple, picking up knife, slicing knife through the apple, and returning knife \cite{Dirk2010}. The approaches of detecting each movement type are analyzed by examining how the features of the data-set were extracted and what learning methods were used to generate a HAR model. Activity recognition accuracies are compared to determine the effectiveness of the machine learning algorithm. The different machine learning algorithms, their number of features, number of activities detected, the kind of sensors used, and the algorithm's accuracy are listed in Table \ref{machine-learning-summary}.
\begin{table}[h]
	\centering
	\caption{Summary of the Reviewed Machine Learning Algorithms}
	\label{machine-learning-summary}
	\begin{tabularx}{\textwidth}{|c|c|c|c|X|c|}
		\hline
		\thead{Algorithm} & \thead{Paper} & \thead{Features} & \thead{Activities} & \thead{Sensors used} & \thead{Accuracy}   \\\hline
		\multicolumn{6}{|l|}{\textbf{Coarse-grained movements}} \\\hline
		\makecell{$k$-NN ($k=1$) \\ $k$-Star} & \cite{Wannenburg2016}  & 29         & 5 & \makecell{accelerometer} & 99.01\% \\\hline
		\makecell{sparse \\ representation} & \cite{Zhang2013} & 60 & 9 & \makecell{accelerometer \\ gyroscope \\ magnetometer} & 96.1\% \\\hline
		\multicolumn{6}{|l|}{\textbf{Fine-grained movements}} \\\hline
		random forest & \cite{Dorfmeister2014} & 80 & 17 & \makecell{accelerometer \\ gyroscope \\ magnetometer \\ barometer \\ GPS \\ microphone} & 90\% \\\hline
		\makecell{conditional \\ random fields} & \cite{Parate2014}  & 24 & 6 & \makecell{accelerometer \\ gyroscope \\ magnetometer} & 95.74\% \\\hline
		$k$-NN ($k=5$) & \cite{Benalcazar2017}  & 8 & 5 & \makecell{accelerometer \\ gyroscope \\ magnetometer \\ EMG} & 86\% \\\hline
		$k$-NN ($k=3$) & \cite{Totty2017}  & 3 & 17 & \makecell{accelerometer \\ gyroscope \\ EMG} & 89.2\% \\\hline
	\end{tabularx}
\end{table}
\subsection{Coarse-grained movements}
A study by Wannenburg and Malekian (2016) detected everyday physical activities using smartphone accelerometer data by applying a $k$-nearest neighbor ($k$-NN) and $k$-Star machine learning algorithm \cite{Wannenburg2016}. Five different activities were recognized: standing, sitting, laying down, walking, and jogging. A three-axis accelerometer sensor on a smartphone placed in ten participant's pants pockets was used to collect the data. Windowing with a size of 1s and 50\% overlap was applied, after normalizing the data. Forty-six features were extracted. The features included the minimum, maximum, mean, and median of every axis, as well as the signal magnitude area (\gls{SMA}). The 29 highest contributing features were selected to train ten different classifiers. $k$-NN ($n = 1$) and $k$-Star achieved the highest classification accuracy, with 99.01\%. Wannenburg and Malekian only detected activities related to daily life. The algorithm's limited number of recognized activities is not usable for automatically detecting procedures administered by EMS personnel. The algorithm's high accuracy signifies a good feature extraction and training process, but the activities are not representative of the coarse-grained movements related to administering EMS procedures.
\par A three-axis accelerometer, three-axis gyroscope, and three-axis magnetometer were used by Zhang and Sawchuk (2013) to recognize nine common activities \cite{Zhang2013}. Fourteen participants performed the activities: walk forward, walk left, walk right, go upstairs, go downstairs, jump up, run, stand, and sit. The sensors were placed on the participant's hip, and 110 features were extracted, such as mean, median, variance, SMA, etc. The features were selected using sequential forward selection for four classifiers: $k$-nearest neighbor, naive Bayesian classifier, support vector machine, and sparse representation. The sparse representation classifier achieved the highest accuracy (96.1\%) when using 60 features.
\par The algorithms of Zhang and Sawchuk \cite{Zhang2013}, and Wannenburg and Malekian \cite{Wannenburg2016} were able to accurately detect common daily life human activities.. The activities detected in Zhang and Sawchuk's work were broader than those detected by Wannenburg and Malekian. The features extracted were similar, such as using min, max, SMA, etc. Therefore, using accelerometer, gyroscope, and magnetometer to detect common human activities in daily life is highly accurate. The same process of processing data of coarse-grained movement can be applied for recognizing procedures administered by EMS personnel.
\subsection{Fine-grained movements}
A machine learning algorithm developed by Maier and Dorfmeister (2014) detected 17 unique fine-grained activities and transportation phases related to subway travel \cite{Dorfmeister2014}. The 17 fine-grained movements are: walking in the subway station, walking upstairs/downstairs, using an escalator (up and down without walking, up and down while walking), using an elevator (up and down), waiting, waiting while the subway arrives, entering the subway train, standing in the subway while parking/accelerating/driving/decelerating, and exiting the subway train. The sensors used were accelerometer, gyroscope, magnetometer, barometer, GPS, and microphone. A window of 2 seconds with 50\% overlap was applied to the sensor data in order to achieve the highest classifier accuracy. The transitions between activities were ignored. The Fast-Fourier transformation was applied in order to the sensor data to compute frequency-based features, while time-based features, i.e., the maximum and the mean, were computed to generate a total of 632 features. A correlation-based feature subset selection was used to filter the 632 features down to 80 features. The random forest classifier achieved a 90\% accuracy with the algorithm's parameters set to their default values. This study shows that fine-grained activities can be accurately detected using the sensors on a smartphone. Microphone data may not be included in a healthcare environment due to privacy concerns.
\par A wrist worn nine-axis inertial measurement unit (IMU) was used to detect smoking gestures \cite{Parate2014}. The IMU device consisted of a three-axis accelerometer, three-axis gyroscope, and three-axis magnetometer. Participants labeled smoking, eating sessions, and "other" using a mobile app. Fine-grained gestures, such as smoking puffs, food bites, and "other" were added a posteriori. Data from 15 participants included 28 hours of 17 smoking sessions, and 10 eating sessions, as well as 369 smoking puffs and 252 food bites. Data labeled "other" was not used in classification. A conditional random field classifier achieved 95.74\% accuracy in detecting smoking puffs and food bites using the data from a ten second window. The system differentiated very similar gestures; however, only two activities were considered. Many different gestures occur in the context of an EMS personnel performing procedures inside of an ambulance.
\par A Myo armband was used to detect hand gestures \cite{Benalcazar2017}, which included: fist, open hand, wave hand in, wave hand out, pinch fingers, and no gesture. Electromyography (\gls{EMG}) data was captured at 200 Hz and pre-processed by taking the absolute value of all of the EMG channels followed by a Butterworth filter to reduce noise and smooth each channel. A 2-second window with an overlap of 50\% was applied to the EMG data, which generates 400 samples. The \emph{k}-nearest neighbor rule and the dynamic time warping algorithm was used to recognize the hand gestures and achieved an accuracy of 86\%. Myo's proprietary hand gesture recognition only achieves an accuracy of 83\% \cite{Benalcazar2017}. Processing the data prior to algorithmic inclusion may be useful for detecting fine-grained movements by EMS personnel when performing procedures on patients.
\par Using data from the Myo armband, Totty and Wade (2017) trained a $k$-NN machine learning algorithm to detect upper-extremity activities \cite{Totty2017}. Gestures were categorized and split into tasks with an approach used by the Functional Arm Activity Behavioral Observation System \cite{FAABOS}. Ten participants performed 17 upper-extremity tasks: arm swaying during walking, assisted movement, touching face, scratching leg, waving, covering yawn, holding object, adjusting arm position, reaching, grabbing, wiping a table, moving an object, transferring an object from hand-to-hand, pushing up from a seated position, wiping a table hurriedly, waving excitedly, and scrubbing. The accelerometer and gyroscope data were smoothed using a 4th order Butterworth band-pass filter, and the EMG data was high-pass filtered. The features included the mean and the \gls{SMA} of the acceleration and gyroscope data, and the root mean square (\gls{RMS}) of the \gls{EMG} data. Data from the magnetometer was not used on the algorithm, due to the sensor's susceptibility to environmental noise \cite{Ahmad2013}. The $k$-NN classifier achieved an accuracy of 89.2\%.
\par Compared to the first Myo study, Totty and Wade detected activities, such as holding an object, reaching, and grabbing may be useful in the context of medical procedure detection. In the medical context an EMS personnel frequently reaches and grabs tools inside an ambulance. The feature extraction and pre-processing methods can be used as a basis for data collected in a study.
\subsection{Intention Recognition}
\par A human's motion can be characterized using the sequence of movements \cite{Dirk2010}. For example, EMS personnel placing an oral airway have to place a thumb on the bottom teeth and index finger on the patient's upper teeth, then move the fingers outward, insert the airway into the patient's mouth, and finally rotate the airway 180 degrees.
\par The human's sequence of actions can be used to predict the next intended action \cite{Schrempf2005}. Recognizing the human's intention based on the motion trajectories is proposed by Huang, Jiang, Chui, and Jiang \cite{Yang2017}. A stacked Hidden Markov Model was trained to recognize the primitive and subtask level during virtual laparoscopic cholecystectomy surgery. The four subtasks were: ablation of the connective tissue and dissection of the cystic duct, checking the clearance between the cystic duct and the liver, deployment of three clips on the cystic duct, and division of the cystic duct. The motion primitive layer considers the motion of both hands individually. Twelve participants performed the virtual surgery ten times. The processing window was 0.5 seconds with 80\% overlap. The recognition rate was 71\% for the subtask level and 95\% for the primitive level. A recognition rate as low as 71\% is impractical when automatically detecting procedures performed by EMS personnel. The intention recognition system only differentiated between four subtasks, while EMS perform many different subtasks in an ambulance.
\par A Hybrid Dynamic Bayesian Network was used to recognize the intended coarse-grained movement \cite{Gehrig2011}. The probability density over all classes was represented using a continuous density function. The recognition system was tested using seven kitchen activities captured on video: set the table, prepare cereal, prepare pudding, eat with a spoon, eat with a fork, clear the table, and wipe the table. The average recognition rate was 74.4\% for the test set. An intention recognition system can send the hospital information regarding the procedure that may be performed next on the patient, while the treatment is still in progress. Communicating time-critical information about the current treatment allows the emergency department to prepare accordingly, before patient arrival.
\subsection{Discussion}
Distinguishing between coarse-grained and fine-grained movements is important in the context of detecting procedures administered by EMS personnel on a patient inside of an ambulance. Many procedures include similar fine-grained movements, which can lead to misclassification. Therefore, procedure recognition must account for both types of movement during classification.
\par Most algorithms followed the same approach for feature extraction and processing data. This approach of taking a window with 50\% overlap, reducing noise with a 4th order Butterworth band-pass filter, and calculating mean, median, variance, SMA, etc. can be applied to data for automatically recognizing procedures administered by EMS personnel. Data from EMS personnel administering procedures needs to be pre-processed to reduce noise and calculate more meaningful metrics. The window size applied to the data depends on the length of a given activity, while existing research shows a 50\% overlap as the most accurate \cite{Wannenburg2016}.
\par An IMU is good at detecting fine-grained movements \cite{Parate2014}. The IMU's accelerometer, gyroscope, and magnetometer have successfully detected coarse-grained movements \cite{Zhang2013}. The Myo, which incorporates acceleration, gyroscope and magnetometer, is useful for detecting muscle activation on a human's arm. The Myo has been proven to be accurate in detecting fine-grained human activities \cite{Benalcazar2017}. A procedure administered by EMS personnel consists of many fine-grained movements. Through detecting the fine-grained movements using the Myo, the procedures can be predicted. The existing literature uses similar approaches to feature extraction and data processing, which can be applied in the context of automatically detecting EMS procedures.

\section{Summary}
\label{sec:Literature-Review:Summary}
External and wearable sensors were examined and compared for their applicability in the medical field. Combining an external sensor with wearable sensors may achieve the highest accuracy for detecting human activities. Human activities consist of coarse-grained and fine-grained movements. Several machine learning algorithms that use wearable sensor data can accurately detect human activities. Inferring coarse-grained movements from fine-grained movements may be useful in the context of detecting procedures administered by EMS personnel inside an ambulance. 